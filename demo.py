import sys

# import math
import numpy as np
# import random
import torch
import zarr
from PIL import Image
from PyQt5.QtCore import Qt, QPoint
from PyQt5.QtGui import QPixmap, QPainter, QPen, QColor
from PyQt5.QtWidgets import QApplication, QMainWindow, QLabel, QPushButton, QTextEdit, QCheckBox, QFrame, QSlider
from scipy.stats import multivariate_normal
# import torch.nn as nn
# from tqdm.auto import tqdm
from skimage.measure import label

from models.segNeuro import segEM2d, segEM3d


# from waterz import get_segmentation


class CustomSlider(QSlider):
    def __init__(self, orientation, parent=None):
        super().__init__(orientation, parent)

    def mouseReleaseEvent(self, event):
        super().mouseReleaseEvent(event)
        # Emit a custom signal or call a function
        self.parent().rawSlider_released(event)


class ImageViewer(QMainWindow):
    def __init__(self):
        super().__init__()
        # UI setting
        self.mode = 0  # 初始模式为0
        self.image_idx = 0
        self.max_image_idx = 32
        self.points = []  # 存储点击的点坐标和颜色

        ##Model setting
        # self.device = 'cpu'
        self.device = 'cuda'
        self.segNeuro2d_model = 'segEM2d(hemi+fib25)_Best_in_val.model'
        self.segNeuro3d_model = 'segEM3d_trace(hemi+fib25)_Best_in_val.model'

        self.crop_size = 512
        self.slices_tracking = 32

        self.initUI()
        self.initModels()

    def initUI(self):
        self.setWindowTitle('Interactive Neuron Segmentation Tool')
        self.setGeometry(100, 100, 1400, 760)

        ##raw interface
        self.raw_slice_show = QLabel(self)
        self.raw_slice_show.setGeometry(10, 10, 600, 600)
        self.raw_slice_show.mousePressEvent = self.mousePressEvent
        # self.raw_slice_show.mouseMoveEvent = self.mouseMoveEvent
        self.raw_slice_show.setMouseTracking(True)
        self.setMouseTracking(True)

        ##raw slider
        self.raw_slice_slider = CustomSlider(Qt.Horizontal, self)
        self.raw_slice_slider.setGeometry(500, 620, 100, 20)
        self.raw_slice_slider.setMinimum(0)
        self.raw_slice_slider.setMaximum(200)
        # self.raw_slice_slider.setSingleStep(1)
        self.raw_slice_slider.setValue(0)
        # self.raw_slice_slider.setTickPosition(QSlider.TicksBelow)
        self.raw_slice_slider.valueChanged.connect(self.rawSlider_valueChange)

        # Label to display the flag value
        self.raw_slice_label = QLabel('Slice: 0', self)
        self.raw_slice_label.setGeometry(420, 612, 60, 30)  # 设置标签的位置和大小

        self.loadImage()

        ##interfence button
        self.inference_btn = QPushButton('Inference', self)
        self.inference_btn.setGeometry(10, 615, 100, 30)
        self.inference_btn.clicked.connect(self.inference)

        ##track button
        self.track_btn = QPushButton('Trace', self)
        self.track_btn.setGeometry(230, 615, 100, 30)
        self.track_btn.clicked.connect(self.track)

        ##Output image interface
        self.seg_show = QLabel(self)
        self.seg_show.setGeometry(650, 10, 600, 600)
        self.seg_show.setFrameShape(QFrame.Box)
        # self.seg_show.mousePressEvent = self.mousePressEvent
        self.segPixmap = QPixmap(600, 600)
        self.segPixmap.fill(Qt.white)
        self.affinityPixmap = QPixmap(600, 600)
        self.affinityPixmap.fill(Qt.white)
        self.seg_show.setPixmap(
            self.segPixmap.scaled(self.seg_show.size(), Qt.KeepAspectRatio, Qt.SmoothTransformation))

        ##Checkbox
        self.mode_checkbox = QCheckBox('ACRLSD', self)
        self.mode_checkbox.setGeometry(120, 615, 100, 30)
        self.mode_checkbox.stateChanged.connect(self.changeMode)

        ##Output text interface
        self.output_text = QTextEdit(self)
        self.output_text.setGeometry(10, 650, 600, 100)

    def initModels(self):
        # set device
        # device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        device = self.device
        print(device)
        self.output_text.append("device:{}".format(device))

        ##Load 2d model
        model_path = './checkpoints/' + self.segNeuro2d_model
        # self.segNeuro2d_model = segneuro2d()
        self.segNeuro2d_model = segEM2d()
        weights = torch.load(model_path, map_location=torch.device('cpu'))
        self.segNeuro2d_model.load_state_dict(weights)
        self.segNeuro2d_model = self.segNeuro2d_model.to(device)
        self.output_text.append("Pretrained 2d model loaded!")
        print("Pretrained 2d model loaded!")

        ##Load 3d model
        model_path = './checkpoints/' + self.segNeuro3d_model
        self.segNeuro3d_model = segEM3d()
        weights = torch.load(model_path, map_location=torch.device('cpu'))
        model_dict = self.segNeuro3d_model.state_dict()
        save_dict = {k: v for k, v in weights.items() if k in model_dict.keys()}
        model_dict.update(save_dict)
        self.segNeuro3d_model.load_state_dict(model_dict)  # weights
        self.segNeuro3d_model = self.segNeuro3d_model.to(device)
        self.output_text.append("Pretrained 3d model loaded!")
        print("Pretrained 3d model loaded!")

    def loadImage(self):
        self.raw = zarr.open('./data/hemibrain_test_roi_1/raw', mode='r')

        ###
        # print("Loading all data to memory...")
        # self.raw = np.array(self.raw)
        print("Loading demo data to memory...")
        self.raw = np.array(self.raw[:101])
        self.raw_slice_slider.setMaximum(self.raw.shape[0] - 1)

        idx = self.image_idx
        test_patch = np.array(self.raw[idx, 200:-200, 200:-200])
        pad_size_x = 8 - (test_patch.shape[0] % 8)
        pad_size_y = 8 - (test_patch.shape[1] % 8)
        test_patch = np.pad(test_patch, ((0, pad_size_x), (0, pad_size_y)), 'constant', constant_values=(0, 0))

        self.pixmap = Image.fromarray(test_patch[:self.crop_size, :self.crop_size]).toqpixmap()
        print("Original shape = {},crop to ({},{})".format(test_patch.shape, self.crop_size, self.crop_size))
        self.raw_slice_show.setPixmap(
            self.pixmap.scaled(self.raw_slice_show.size(), Qt.KeepAspectRatio, Qt.SmoothTransformation))

    def mousePressEvent(self, event):
        x = event.x()
        y = event.y()
        if x > 10 and x < 610 and y > 10 and y < 610:
            if event.button() == Qt.LeftButton:
                self.points.append((x, y, QColor("green"), 1))
            elif event.button() == Qt.RightButton:
                self.points.append((x, y, QColor("red"), 0))
        self.update()

    def mouseMoveEvent(self, event):
        x = event.x()
        y = event.y()
        if x > 10 and x < 610 and y > 10 and y < 610:
            if self.mode == 0:
                self.seg_show.setPixmap(
                    self.segPixmap.scaled(self.seg_show.size(), Qt.KeepAspectRatio, Qt.SmoothTransformation))
            else:
                self.seg_show.setPixmap(
                    self.affinityPixmap.scaled(self.seg_show.size(), Qt.KeepAspectRatio, Qt.SmoothTransformation))
            painter = QPainter(self.seg_show.pixmap())
            pen = QPen()
            pen.setWidth(8)
            pen.setColor(QColor("blue"))
            painter.setPen(pen)
            # print("({},{})".format(x,y))
            painter.drawPoint(QPoint(x, y))
            painter.end()
        self.update()

    def keyPressEvent(self, event):
        if event.key() == Qt.Key_Q and len(self.points) > 0:
            self.points.pop()
        elif event.key() == Qt.Key_E:
            self.points = list()
        # print(len(self.points))
        self.raw_slice_show.setPixmap(
            self.pixmap.scaled(self.raw_slice_show.size(), Qt.KeepAspectRatio, Qt.SmoothTransformation))
        self.paintEvent(event)

    def changeMode(self, state):
        if state == Qt.Checked:
            self.mode = 1
            self.seg_show.setPixmap(
                self.affinityPixmap.scaled(self.seg_show.size(), Qt.KeepAspectRatio, Qt.SmoothTransformation))
        else:
            self.mode = 0
            self.seg_show.setPixmap(
                self.segPixmap.scaled(self.seg_show.size(), Qt.KeepAspectRatio, Qt.SmoothTransformation))

    def paintEvent(self, event):
        painter = QPainter(self.raw_slice_show.pixmap())
        pen = QPen()
        pen.setWidth(8)

        for point in self.points:
            pen.setColor(point[2])
            painter.setPen(pen)
            # painter.setBrush(point[2])

            painter.drawPoint(QPoint(point[0], point[1]))

    def rawSlider_valueChange(self, value):
        self.image_idx = value
        self.raw_slice_label.setText(f'Slice: {value}')

    def rawSlider_released(self, event):
        # print("Relseased")
        idx = self.image_idx
        test_patch = np.array(self.raw[idx, 200:-200, 200:-200])
        # test_patch = np.array(self.raw[200+idx,200:712,200:712])
        pad_size_x = 8 - (test_patch.shape[0] % 8)
        pad_size_y = 8 - (test_patch.shape[1] % 8)
        test_patch = np.pad(test_patch, ((0, pad_size_x), (0, pad_size_y)), 'constant', constant_values=(0, 0))

        self.pixmap = Image.fromarray(test_patch[:self.crop_size, :self.crop_size]).toqpixmap()
        print("Original shape = {},crop to ({},{})".format(test_patch.shape, self.crop_size, self.crop_size))
        self.raw_slice_show.setPixmap(
            self.pixmap.scaled(self.raw_slice_show.size(), Qt.KeepAspectRatio, Qt.SmoothTransformation))

    ##Inference-related functions
    def normalize(self, data):
        return (data - np.min(data)) / (np.max(data) - np.min(data)).astype(np.float32)

    def generate_gaussian_matrix(self, Points_pos, Points_lab, H, W, theta=10):
        total_matrix = np.zeros((H, W))

        record_list = list()
        for n, (X, Y) in enumerate(Points_pos):
            if (X, Y) not in record_list:
                record_list.append((X, Y))
            else:
                continue

            # 生成坐标网格
            x, y = np.meshgrid(np.arange(W), np.arange(H))
            pos = np.dstack((x, y))

            # 以 (X, Y) 为中心，theta 为标准差生成高斯分布
            rv = multivariate_normal(mean=[X, Y], cov=[[theta, 0], [0, theta]])

            # 计算高斯分布在每个像素上的值
            matrix = rv.pdf(pos)
            ##normalize
            matrix = matrix * (1 / np.max(matrix))

            total_matrix = total_matrix + matrix * (Points_lab[n] * 2 - 1)
            # total_matrix = total_matrix + matrix

        if np.max(Points_lab) == 0:
            total_matrix = total_matrix * 2 + 1

        return total_matrix

    def create_lut(self, labels):
        ##labels标记了图像中所有的连通域，维度对应图像的维度
        max_label = np.max(labels)
        ## 下面这个lut是给每个连通域随机设定rgb值
        np.random.seed(100)
        lut = np.random.randint(
            low=0,
            high=255,
            size=(int(max_label + 1), 3),
            dtype=np.uint8)
        lut[0] = 0
        ## 除了rgb外，增加一个通道，lut的维度为：连通域数量*4
        lut = np.append(
            lut,
            np.zeros(
                (int(max_label + 1), 1),
                dtype=np.uint8) + 255,
            axis=1)

        lut[0] = 0  ##第0个连通域为背景，所有通道值设为0
        colored_labels = lut[labels]  ## 获得所有像素点的各通道值，维度为：图像维度*通道数（4）

        return colored_labels

    def inference(self):
        ##Load image
        idx = self.image_idx
        raw_slice0 = np.array(self.raw[idx, 200:-200, 200:-200])
        # raw_slice0 = np.array(self.raw[200+idx,200:712,200:712])
        pad_size_x = 8 - (raw_slice0.shape[0] % 8)
        pad_size_y = 8 - (raw_slice0.shape[1] % 8)
        raw_slice0 = np.pad(raw_slice0, ((0, pad_size_x), (0, pad_size_y)), 'constant', constant_values=(0, 0))
        raw_slice0 = self.normalize(raw_slice0[:self.crop_size, :self.crop_size])

        # if self.mode == 1:
        #     ##Inference
        #     raw = torch.tensor(raw_slice0 ,dtype=torch.float32).unsqueeze(0).unsqueeze(1).to(self.device)
        #     y_lsds,y_affinity = self.segNeuro2d_model.model_affinity(raw)
        #     y_affinity = y_affinity.detach().cpu().numpy()

        #     ######计算ACRLSD的性能
        #     threshold = [0.35] #higher thresholds will merge more, lower thresholds will split more
        #     ##1: Segment upper
        #     print('Start watershed process...')
        #     # watershed assumes 3d arrays, create fake channel dim (call these watershed affs - ws_affs)
        #     # affs shape: 3, h, w
        #     # waterz agglomerate requires 4d affs (c, d, h, w) - add fake z dim
        #     ws_affs = (1-y_affinity).transpose(1,0,2,3)
        #     ws_affs = np.stack([
        #             ws_affs[0],
        #             ws_affs[1]]
        #         )

        #     seg_acrlsd = get_segmentation(ws_affs, threshold)
        #     print(seg_acrlsd.shape)

        # else:
        Points_pos = list()
        Points_lab = list()
        self.output_text.append("Inference once,the prompts are as follows:")
        print("推理一次,prompt信息如下:")
        for point in self.points:
            x_img = int((point[0] - 10) / 600 * self.crop_size)
            y_img = int((point[1] - 10) / 600 * self.crop_size)

            # print("({},{}):{}".format(point[0],point[1],point[3]))
            self.output_text.append("({},{}):{}".format(x_img, y_img, point[3]))

            Points_pos.append([x_img, y_img])
            Points_lab.append(point[3])
        Points_pos = np.array(Points_pos)
        Points_lab = np.array(Points_lab)

        ##Inference
        if len(self.points) == 0:
            point_map = torch.ones(raw_slice0.shape, dtype=torch.float32).unsqueeze(0).to(self.device)
        else:
            point_map = self.generate_gaussian_matrix(Points_pos, Points_lab, raw_slice0.shape[0], raw_slice0.shape[1],
                                                      theta=30)
            point_map = torch.tensor(point_map, dtype=torch.float32).unsqueeze(0).to(self.device)
        self.point_map = point_map

        raw = torch.tensor(raw_slice0, dtype=torch.float32).unsqueeze(0).unsqueeze(1).to(self.device)
        y_mask, y_lsds, y_affinity = self.segNeuro2d_model(raw, self.point_map)
        # y_mask,y_affinity = self.segNeuro2d_model(raw,self.point_map)
        y_mask = (y_mask > 0.5) + 0
        segmentation = label((y_mask.squeeze().detach().cpu().numpy())[:, :])
        segmentation_color = self.create_lut(segmentation)
        # segmentation_color[:,:,0] = (raw_slice0*255).astype(np.uint8)

        ##
        y_affinity = np.sum(y_affinity.detach().cpu().numpy().squeeze(), axis=0)
        y_affinity = ((y_affinity - np.min(y_affinity)) / (np.max(y_affinity) - np.min(y_affinity)) * 255).astype(
            np.uint8)
        print(np.min(y_affinity))
        trans = Image.fromarray(y_affinity)
        self.affinityPixmap = trans.toqpixmap()

        # pixmap = QPixmap(self.seg_show_path)
        trans = Image.fromarray(segmentation_color)
        self.segPixmap = trans.toqpixmap()

        if self.mode == 0:
            self.seg_show.setPixmap(
                self.segPixmap.scaled(self.seg_show.size(), Qt.KeepAspectRatio, Qt.SmoothTransformation))
        else:
            self.seg_show.setPixmap(
                self.affinityPixmap.scaled(self.seg_show.size(), Qt.KeepAspectRatio, Qt.SmoothTransformation))

    def track(self):
        ##Load image
        self.raw_3d = np.zeros((self.crop_size, self.crop_size, self.slices_tracking), dtype=np.float32)
        # for idx in range(self.slices_tracking):
        if (self.image_idx + self.slices_tracking) >= self.raw.shape[0]:
            self.image_idx = 0
            self.output_text.append("Error tracing. Out of raw shape! Slice reset to 0")
        for idx in range(self.image_idx, self.image_idx + self.slices_tracking):
            raw_slice0 = np.array(self.raw[idx, 200:-200, 200:-200])
            pad_size_x = 8 - (raw_slice0.shape[0] % 8)
            pad_size_y = 8 - (raw_slice0.shape[1] % 8)
            raw_slice0 = np.pad(raw_slice0, ((0, pad_size_x), (0, pad_size_y)), 'constant', constant_values=(0, 0))
            self.raw_3d[:, :, idx - self.image_idx] = raw_slice0[:self.crop_size, :self.crop_size]
        self.raw_3d = self.normalize(self.raw_3d)

        raw = torch.as_tensor(self.raw_3d, dtype=torch.float, device=self.device)
        raw = raw.unsqueeze(0).unsqueeze(0)

        print("raw shape = {}".format(raw.shape))
        print("self.point_map shape = {}".format(self.point_map.shape))
        y_mask, _, _, _ = self.segNeuro3d_model(raw, self.point_map)
        # y_mask,_,_ =  self.segNeuro3d_model(raw,self.point_map)
        y_mask = (y_mask > 0.5) + 0
        y_mask = y_mask.squeeze().detach().cpu().numpy().transpose(2, 0, 1)
        segmentation = label(y_mask)
        segmentation_color = self.create_lut(segmentation)

        import imageio
        print(segmentation_color.shape)
        # np.save('./output/y_seg3d_color.npy',y_seg3d_color)
        imageio.volwrite('./output/roi1_slices0-31.tiff', segmentation_color)
        self.output_text.append("Tracing ends!")


if __name__ == '__main__':
    app = QApplication(sys.argv)

    viewer = ImageViewer()
    viewer.show()
    sys.exit(app.exec_())
